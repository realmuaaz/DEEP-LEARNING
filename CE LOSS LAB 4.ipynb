import numpy as np

# Ground truth labels (one-hot encoded)
y_true = np.array([
    [1, 0, 0, 0, 0],  # Sample 1: Class 1
    [0, 1, 0, 0, 0],  # Sample 2: Class 2
    [0, 0, 1, 0, 0],   # Sample 3: Class 3
    [0, 0, 0, 1, 0],  # Sample 2: Class 2
    [0, 0, 0, 0, 1],
])

# Model predictions (logits)
logits = np.array([
    [2.0, 1.0, 0.1, 0.3, 1.5],  # Sample 1
    [0.5, 2.5, 0.2, 1.5, 1.0],  # Sample 2
    [0.1, 0.2, 3.0, 0.4, 5.0],   # Sample 3
    [0.5, 0.2, 3.5, 0.8, 1.0],
    [0.4, 2.2, 3.5, 1.4, 2.0], 
])

# Softmax function
def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # Numerical stability
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

# Compute softmax probabilities
probs = softmax(logits)
print("Softmax Probabilities:\n", probs)

# Compute log(probs)
log_probs = np.log(probs)
print("\nLog of Softmax Probabilities:\n", log_probs)

# Compute y_true * log(probs)
y_true_log_probs = y_true * log_probs
print("\ny_true * log(probs):\n", y_true_log_probs)

# Compute CE loss for each sample
ce_loss = -np.sum(y_true_log_probs, axis=-1)
print("\nCE Loss per Sample:", ce_loss)

# Average CE loss for the dataset
avg_ce_loss = np.mean(ce_loss)
print("\nAverage CE Loss for the Dataset:", avg_ce_loss)


Softmax Probabilities:
 [[0.43352684 0.15948561 0.06484201 0.07919821 0.26294732]
 [0.07409121 0.54746412 0.05488812 0.20140079 0.12215576]
 [0.00641359 0.00708811 0.11656149 0.00865744 0.86127937]
 [0.04028208 0.0298417  0.80908718 0.05437512 0.06641392]
 [0.02708639 0.1638631  0.60126232 0.07362844 0.13415976]]

Log of Softmax Probabilities:
 [[-0.83580156 -1.83580156 -2.73580156 -2.53580156 -1.33580156]
 [-2.60245836 -0.60245836 -2.90245836 -1.60245836 -2.10245836]
 [-5.04933635 -4.94933635 -2.14933635 -4.74933635 -0.14933635]
 [-3.2118486  -3.5118486  -0.2118486  -2.9118486  -2.7118486 ]
 [-3.60872397 -1.80872397 -0.50872397 -2.60872397 -2.00872397]]

y_true * log(probs):
 [[-0.83580156 -0.         -0.         -0.         -0.        ]
 [-0.         -0.60245836 -0.         -0.         -0.        ]
 [-0.         -0.         -2.14933635 -0.         -0.        ]
 [-0.         -0.         -0.         -2.9118486  -0.        ]
 [-0.         -0.         -0.         -0.         -2.00872397]]

CE Loss per Sample: [0.83580156 0.60245836 2.14933635 2.9118486  2.00872397]

Average CE Loss for the Dataset: 1.701633767870576
